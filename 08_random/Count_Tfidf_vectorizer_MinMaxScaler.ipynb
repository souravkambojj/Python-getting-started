{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Count/Tfidf_vectorizer/MinMaxScaler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNha2vwHSLis"
      },
      "source": [
        "text = ['This is very powerful, but is also very dangerous if you accept strings to evaluate from untrusted input. Suppose the string being evaluated is  It will really start deleting all the files on your computer',\r\n",
        "        'Safely evaluate an expression node or a string containing a Python literal or container display. The string or node provided may only consist of the following Python literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, None, bytes and sets.',\r\n",
        "        'The string or node provided may only consist of the following Python literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, and None',\r\n",
        "        'means that you actually evaluate the code before you deem it to be unsafe or not. It evaluates the code as soon as the function is called.',\r\n",
        "        'I read through the docs and I am still unclear if this would be safe or not. Does eval evaluate the data as soon as its entered or after the datamap variable is called?']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo1QHmc86DNa"
      },
      "source": [
        "## **CountVectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U43KQ_lb6A05"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rdl7aNP6YRN"
      },
      "source": [
        "# it creates a 'document-term matrix'\r\n",
        "#columns represents the unique tokens and each row represents a document.\r\n",
        "#Each cell represents frequency of each word corresponding to that document.\r\n",
        "#like, w1 occured thrice, twice and once in doc d1,d2,d3 resp."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm-Qer8u7ZnJ"
      },
      "source": [
        "#      w1  w2 w3 ...             #w1,w2,w3 unique tokens throughout the documents\r\n",
        "#   d1  2  0  1  ...             #d1,d2,d3 documents\r\n",
        "#   d2  1  1  3  ...\r\n",
        "#   d3  3  2  1  ...\r\n",
        "#   .   .  .  .\r\n",
        "#   .   .  .  .\r\n",
        "#   .   .  .  ."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBewJnyS8Db5"
      },
      "source": [
        "cv = CountVectorizer()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9jpEgp_8U2J"
      },
      "source": [
        "x = cv.fit(text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srTWlqNy8aNG"
      },
      "source": [
        "#x.vocabulary_     #vocab of unique words nd their corresponding indices"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPb_S8f680Ib"
      },
      "source": [
        "y = cv.transform(text)\r\n",
        "#it transform the text documents to document-term- matrix"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yef5lDCw9USb",
        "outputId": "988bdef3-5a11-420f-8c85-757db151584a"
      },
      "source": [
        "y.shape   #5 documents and 87 unique tokens"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 87)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9xIAWa89VRx",
        "outputId": "f43f1adb-be7a-4bcb-ffd3-9a05690d685e"
      },
      "source": [
        "y.toarray()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 3, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "        1, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, 1, 0,\n",
              "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 2, 1, 0, 2, 1, 0, 1, 1, 0, 1, 3, 0, 1, 2, 0, 0, 0, 1, 2, 0,\n",
              "        0, 0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "        0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2,\n",
              "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "        0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0],\n",
              "       [0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
              "        0, 1, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW6WLNh69rAx"
      },
      "source": [
        "## **TDIDF Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qcqZm9L9gbN"
      },
      "source": [
        "# it creates a 'document-term matrix'\r\n",
        "# columns represents the unique tokens and each row represents a document.\r\n",
        "# each cell contains a tf-idf weight which signifies how imp a word is for an individual document.\r\n",
        "\r\n",
        "# TF-IDF = TF*IDF\r\n",
        "\r\n",
        "#Term-Frequency: which measures how frequently a term occurs in a document. Since every document is different in length,\r\n",
        "#it is possible that a term would appear much more times in long documents than shorter ones. thus term-freq is often\r\n",
        "#divided by the document length as a way of normalization:\r\n",
        "\r\n",
        "#TF(t,d) = (No of times term 't' appears in document(d))/(Total no terms in document(j)).\r\n",
        "\r\n",
        "#Inverse Document Frequency: it measures how imp the term is. while computing TF, all terms are considered equally imp. However it is known \r\n",
        "#that certain terms, such as 'is','of','that', amy appear a lot of times but have very little importance. thus we need to weight down the \r\n",
        "#frequent terms while scale up the rare ones, by computing the following:\r\n",
        "\r\n",
        "#IDF(t) = Log_e(Total no of documents/No of documents in which term 't' appeared).\r\n",
        "\r\n",
        "\r\n",
        "#consider a document containing 100 words wherein the word cat appeared 3 times\r\n",
        "#we have total 10M documents the word cat appears in 1000 of times.\r\n",
        "\r\n",
        "# tf = 3/100 = 0.3\r\n",
        "#idf = log(10**7/10**3) = log(10**4) = 4\r\n",
        "#TF-IDF = 0.3*4 = 0.12"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkLz1kMGBnKp"
      },
      "source": [
        "# if in a given doument a term is occuring very frequently-----TF will be high.\r\n",
        "# if a term is rarely occuring in a document then numerator will be low but idf will be high.\r\n",
        "\r\n",
        "#conclusion: if a term ocuurs frequently in a given document but its a rarely occuring in other documents.\r\n",
        "#TF-IDF weight of that term for given document will be large"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxX7kzJlC6YH"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsEt_FHqDE7Y"
      },
      "source": [
        "tf = TfidfVectorizer()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P49OYeQ_DQ6K"
      },
      "source": [
        "tx = tf.fit(text)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0hsDM25DSUq"
      },
      "source": [
        "#tx.vocabulary_"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TipTj_rdDZ9t"
      },
      "source": [
        "ty = tf.transform(text)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re7hUGOQDzxM",
        "outputId": "fa15ec70-c063-460b-8cb2-bad4d4577ef7"
      },
      "source": [
        "ty.shape   #5 documents and 87 unique tokens"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 87)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gXFabf1D5zJ",
        "outputId": "e66b273c-5aa7-49eb-d8f3-e262c7741903"
      },
      "source": [
        "ty.toarray()[0]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.17299624, 0.        , 0.        , 0.17299624, 0.17299624,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.17299624, 0.        ,\n",
              "       0.        , 0.        , 0.17299624, 0.        , 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.        , 0.        ,\n",
              "       0.17299624, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.09746307, 0.17299624, 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.17299624, 0.        ,\n",
              "       0.13957228, 0.17299624, 0.3475728 , 0.13957228, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.17299624,\n",
              "       0.        , 0.        , 0.17299624, 0.        , 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.1158576 , 0.1158576 ,\n",
              "       0.        , 0.17299624, 0.        , 0.16486727, 0.13957228,\n",
              "       0.        , 0.13957228, 0.        , 0.        , 0.        ,\n",
              "       0.17299624, 0.        , 0.34599249, 0.17299624, 0.        ,\n",
              "       0.13957228, 0.17299624])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JLUurieEwtH"
      },
      "source": [
        "## **MinMaxScaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_ctUC6AEBSF"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpO9qaFcE4br",
        "outputId": "a7cfe3ec-a5f7-4941-9fc5-00794f51e6f3"
      },
      "source": [
        "data = np.random.randint(1,100,(10,2))\r\n",
        "data"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[64, 17],\n",
              "       [25, 98],\n",
              "       [41, 67],\n",
              "       [11, 19],\n",
              "       [64, 39],\n",
              "       [56, 31],\n",
              "       [79, 48],\n",
              "       [29, 54],\n",
              "       [73, 21],\n",
              "       [91, 92]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UhoYcUEFGLC",
        "outputId": "caae045c-0d83-42dc-adf7-2c59106f7078"
      },
      "source": [
        "mm = MinMaxScaler()\r\n",
        "mm.fit_transform(data)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6625    , 0.        ],\n",
              "       [0.175     , 1.        ],\n",
              "       [0.375     , 0.61728395],\n",
              "       [0.        , 0.02469136],\n",
              "       [0.6625    , 0.27160494],\n",
              "       [0.5625    , 0.17283951],\n",
              "       [0.85      , 0.38271605],\n",
              "       [0.225     , 0.45679012],\n",
              "       [0.775     , 0.04938272],\n",
              "       [1.        , 0.92592593]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzgjxhOwFUQR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}