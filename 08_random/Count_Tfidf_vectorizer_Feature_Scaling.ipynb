{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Count/Tfidf_vectorizer/Feature_Scaling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNha2vwHSLis"
      },
      "source": [
        "text = ['This is very powerful, but is also very dangerous if you accept strings to evaluate from untrusted input. Suppose the string being evaluated is  It will really start deleting all the files on your computer',\n",
        "        'Safely evaluate an expression node or a string containing a Python literal or container display. The string or node provided may only consist of the following Python literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, None, bytes and sets.',\n",
        "        'The string or node provided may only consist of the following Python literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, and None',\n",
        "        'means that you actually evaluate the code before you deem it to be unsafe or not. It evaluates the code as soon as the function is called.',\n",
        "        'I read through the docs and I am still unclear if this would be safe or not. Does eval evaluate the data as soon as its entered or after the datamap variable is called?']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo1QHmc86DNa"
      },
      "source": [
        "## **CountVectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U43KQ_lb6A05"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rdl7aNP6YRN"
      },
      "source": [
        "# it creates a 'document-term matrix'\n",
        "#columns represents the unique tokens and each row represents a document.\n",
        "#Each cell represents frequency of each word corresponding to that document.\n",
        "#like, w1 occured thrice, twice and once in doc d1,d2,d3 resp."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm-Qer8u7ZnJ"
      },
      "source": [
        "#      w1  w2 w3 ...             #w1,w2,w3 unique tokens throughout the documents\n",
        "#   d1  2  0  1  ...             #d1,d2,d3 documents\n",
        "#   d2  1  1  3  ...\n",
        "#   d3  3  2  1  ...\n",
        "#   .   .  .  .\n",
        "#   .   .  .  .\n",
        "#   .   .  .  ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBewJnyS8Db5"
      },
      "source": [
        "cv = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9jpEgp_8U2J"
      },
      "source": [
        "x = cv.fit(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srTWlqNy8aNG"
      },
      "source": [
        "#x.vocabulary_     #vocab of unique words nd their corresponding indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPb_S8f680Ib"
      },
      "source": [
        "y = cv.transform(text)\n",
        "#it transform the text documents to document-term- matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yef5lDCw9USb",
        "outputId": "988bdef3-5a11-420f-8c85-757db151584a"
      },
      "source": [
        "y.shape   #5 documents and 87 unique tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 87)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9xIAWa89VRx",
        "outputId": "f43f1adb-be7a-4bcb-ffd3-9a05690d685e"
      },
      "source": [
        "y.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 3, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "        1, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, 1, 0,\n",
              "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 2, 1, 0, 2, 1, 0, 1, 1, 0, 1, 3, 0, 1, 2, 0, 0, 0, 1, 2, 0,\n",
              "        0, 0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "        0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2,\n",
              "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "        0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0],\n",
              "       [0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
              "        0, 1, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW6WLNh69rAx"
      },
      "source": [
        "## **TDIDF Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qcqZm9L9gbN"
      },
      "source": [
        "# it creates a 'document-term matrix'\n",
        "# columns represents the unique tokens and each row represents a document.\n",
        "# each cell contains a tf-idf weight which signifies how imp a word is for an individual document.\n",
        "\n",
        "# TF-IDF = TF*IDF\n",
        "\n",
        "#Term-Frequency: which measures how frequently a term occurs in a document. Since every document is different in length,\n",
        "#it is possible that a term would appear much more times in long documents than shorter ones. thus term-freq is often\n",
        "#divided by the document length as a way of normalization:\n",
        "\n",
        "#TF(t,d) = (No of times term 't' appears in document(d))/(Total no terms in document(j)).\n",
        "\n",
        "#Inverse Document Frequency: it measures how imp the term is. while computing TF, all terms are considered equally imp. However it is known \n",
        "#that certain terms, such as 'is','of','that', amy appear a lot of times but have very little importance. thus we need to weight down the \n",
        "#frequent terms while scale up the rare ones, by computing the following:\n",
        "\n",
        "#IDF(t) = Log_e(Total no of documents/No of documents in which term 't' appeared).\n",
        "\n",
        "\n",
        "#consider a document containing 100 words wherein the word cat appeared 3 times\n",
        "#we have total 10M documents the word cat appears in 1000 of times.\n",
        "\n",
        "# tf = 3/100 = 0.3\n",
        "#idf = log(10**7/10**3) = log(10**4) = 4\n",
        "#TF-IDF = 0.3*4 = 0.12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkLz1kMGBnKp"
      },
      "source": [
        "# if in a given doument a term is occuring very frequently-----TF will be high.\n",
        "# if a term is rarely occuring in a document then numerator will be low but idf will be high.\n",
        "\n",
        "#conclusion: if a term ocuurs frequently in a given document but its a rarely occuring in other documents.\n",
        "#TF-IDF weight of that term for given document will be large"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxX7kzJlC6YH"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsEt_FHqDE7Y"
      },
      "source": [
        "tf = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P49OYeQ_DQ6K"
      },
      "source": [
        "tx = tf.fit(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0hsDM25DSUq"
      },
      "source": [
        "#tx.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TipTj_rdDZ9t"
      },
      "source": [
        "ty = tf.transform(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re7hUGOQDzxM",
        "outputId": "fa15ec70-c063-460b-8cb2-bad4d4577ef7"
      },
      "source": [
        "ty.shape   #5 documents and 87 unique tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 87)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gXFabf1D5zJ",
        "outputId": "e66b273c-5aa7-49eb-d8f3-e262c7741903"
      },
      "source": [
        "ty.toarray()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.17299624, 0.        , 0.        , 0.17299624, 0.17299624,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.17299624, 0.        ,\n",
              "       0.        , 0.        , 0.17299624, 0.        , 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.        , 0.        ,\n",
              "       0.17299624, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.09746307, 0.17299624, 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.17299624, 0.        ,\n",
              "       0.13957228, 0.17299624, 0.3475728 , 0.13957228, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.17299624,\n",
              "       0.        , 0.        , 0.17299624, 0.        , 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.17299624, 0.        , 0.1158576 , 0.1158576 ,\n",
              "       0.        , 0.17299624, 0.        , 0.16486727, 0.13957228,\n",
              "       0.        , 0.13957228, 0.        , 0.        , 0.        ,\n",
              "       0.17299624, 0.        , 0.34599249, 0.17299624, 0.        ,\n",
              "       0.13957228, 0.17299624])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRxFeaZfyHF"
      },
      "source": [
        "# **Feature Scaling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRUFYikVgJry"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UypYmYRPePQb"
      },
      "source": [
        "#StandardScaler follows Standard Normal/Gaussian Distribution. Therefore, it makes mean = 0 and scales the data to unit variance."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA8oBkLteh6E"
      },
      "source": [
        "#Since, it doesn't restict the data to a particular range.\n",
        "#therefore it cannot guarantee balanced feature scales in the presence of outliers."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ6HSWWybRzZ"
      },
      "source": [
        "#MinMaxScaler scales all the data features in the range [0, 1] or else in the range [-1, 1]\n",
        "#Unlike, StandardScaler it handles outliers."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY-rzh-tfopj"
      },
      "source": [
        "#By using RobustScaler, we can remove the outliers and then use either StandardScaler or MinMaxScaler for preprocessing the dataset."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FxUin2_hkQ-"
      },
      "source": [
        "# **RobustScaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ge1o2StLhjQl",
        "outputId": "1c6d07e8-bef7-454a-a8aa-45a831bfd57d"
      },
      "source": [
        "df = pd.DataFrame({'x1': np.concatenate([np.random.normal(20, 2, 1000), np.random.normal(1, 2, 25)]), \n",
        "    'x2':np.concatenate([np.random.normal(30, 2, 1000), np.random.normal(50, 2, 25)])})\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19.188289</td>\n",
              "      <td>30.582851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.549039</td>\n",
              "      <td>29.630032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20.779547</td>\n",
              "      <td>30.876867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19.891867</td>\n",
              "      <td>29.157373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.864360</td>\n",
              "      <td>29.409329</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          x1         x2\n",
              "0  19.188289  30.582851\n",
              "1  18.549039  29.630032\n",
              "2  20.779547  30.876867\n",
              "3  19.891867  29.157373\n",
              "4  19.864360  29.409329"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PakRuZbziOEm",
        "outputId": "18bbde05-911f-4b9a-e566-14ba9b6a211a"
      },
      "source": [
        "Rs = RobustScaler()\n",
        "Rs.fit_transform(df)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.25295481,  0.19446726],\n",
              "       [-0.49084905, -0.14568502],\n",
              "       [ 0.33922536,  0.29943002],\n",
              "       ...,\n",
              "       [-6.55243104,  6.44357673],\n",
              "       [-7.04053537,  7.48424699],\n",
              "       [-7.40641167,  7.77268364]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB1FwjUmfVgU"
      },
      "source": [
        "## **StandardScaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPlIvYqKih_y",
        "outputId": "28d6eac9-4d6a-4efc-c893-fd932be5d48e"
      },
      "source": [
        "Ss = StandardScaler()\n",
        "Ss.fit_transform(df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.08424744,  0.04118484],\n",
              "       [-0.26127059, -0.22461182],\n",
              "       [ 0.35640887,  0.12320326],\n",
              "       ...,\n",
              "       [-4.77184779,  4.92426959],\n",
              "       [-5.13505863,  5.73745443],\n",
              "       [-5.40731648,  5.96284024]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JLUurieEwtH"
      },
      "source": [
        "## **MinMaxScaler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UhoYcUEFGLC",
        "outputId": "3838fa96-8e69-4707-baab-5d3b1cbf09e8"
      },
      "source": [
        "mm = MinMaxScaler()\n",
        "mm.fit_transform(df)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.77969025, 0.23878959],\n",
              "       [0.75959333, 0.20519302],\n",
              "       [0.82971667, 0.24915668],\n",
              "       ...,\n",
              "       [0.24752068, 0.85600915],\n",
              "       [0.20628642, 0.95879532],\n",
              "       [0.17537778, 0.98728398]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzgjxhOwFUQR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}