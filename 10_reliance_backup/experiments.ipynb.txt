{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "import math\n",
    "from imutils import face_utils\n",
    "from head_pose import get_points_from_landmarks\n",
    "from head_pose import HeadPoseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "\tfacial_features_model = '/home/ubuntu/sourav_kamboj/jupyter_notebook_files/video_analysis/shape_predictor_68_face_landmarks.dat'\n",
    "\tdetector = dlib.get_frontal_face_detector()\n",
    "\tpredictor = dlib.shape_predictor(facial_features_model)\n",
    "\treturn detector, predictor\n",
    "\n",
    "detector, predictor = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(frame, detector=detector, predictor=predictor):\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\trects = detector(gray, 0)\n",
    "\n",
    "\tif str(rects) == 'rectangles[]':\n",
    "\t\treturn None\n",
    "\n",
    "\tfor (i, rect) in enumerate(rects):\n",
    "\t\tlandmarks = predictor(gray, rect)\n",
    "\t\tlandmarks = face_utils.shape_to_np(landmarks)\n",
    "\treturn landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation_threshold(pitch, yaw):\n",
    "\tdirection = []\n",
    "\n",
    "\tif pitch == 0 and yaw == 0:\n",
    "\t    looking = ['Straight']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\t    \n",
    "\t## Pitch - Up & Down\n",
    "\tif pitch > 0 and pitch < 20:\n",
    "\t    looking = ['Normal Up']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\telif pitch > 21 and pitch < 40:\n",
    "\t    looking = ['Moderate Up']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\telif pitch > 41:\n",
    "\t    looking = ['Extreme Up']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\t    \n",
    "\telif pitch > -20 and pitch < 0:\n",
    "\t    looking = ['Normal Down']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\telif pitch > -40 and pitch < -21:\n",
    "\t    looking = ['Moderate Down']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\telif pitch < -41:\n",
    "\t    looking = ['Extreme Down']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\n",
    "\t## Yaw - Left & Right\n",
    "\tif yaw > 0 and yaw < 20:\n",
    "\t    looking = ['Normal Right']\n",
    "\t    direction.append(looking)\n",
    "\n",
    "\telif yaw > 21 and yaw < 40:\n",
    "\t    looking = ['Moderate Right']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\telif yaw > 41:\n",
    "\t    looking = ['Extreme Right']\n",
    "\t    \n",
    "\n",
    "\telif yaw < 0 and yaw > -40:\n",
    "\t    looking = ['Normal Left']\n",
    "\t    direction.append(looking)\n",
    "\n",
    "\telif yaw < -21 and yaw > -40:\n",
    "\t    looking = ['Moderate Left']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\telif yaw < -41:\n",
    "\t    looking = ['Extreme Left']\n",
    "\t    direction.append(looking)\n",
    "\t    \n",
    "\treturn direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orientation(image_path, draw_direction=False, draw_values=False, mode='nose_chin_eyes_mouth'):\n",
    "\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255), (255, 255, 0), (255, 125, 125)]\n",
    "    \n",
    "    image=cv2.imread(image_path)\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    ## Model points\n",
    "    model_points = np.array([\n",
    "                            (0.0, 0.0, 0.0),             # Nose tip\n",
    "                            (0.0, -330.0, -65.0),        # Chin\n",
    "                           (-165.0, 170.0, -135.0),     # Left eye left corner\n",
    "                            (165.0, 170.0, -135.0),      # Right eye right corne\n",
    "                            (-150.0, -150.0, -125.0),    # Left Mouth corner\n",
    "                            (150.0, -150.0, -125.0)      # Right mouth corner                         \n",
    "                        ])\n",
    "\n",
    "    ## Center of image\n",
    "    center = (image.shape[1]/2, image.shape[0]/2)\n",
    "\n",
    "    ## Focal Length (approx.)\n",
    "    focal_length = center[0] / np.tan(60/2 * np.pi / 180)\n",
    "\n",
    "    ## Camera matrix\n",
    "    camera_matrix = np.array([[focal_length, 0, center[0]],\n",
    "\t\t\t\t\t\t\t [0, focal_length, center[1]],\n",
    "\t\t\t\t\t\t\t [0, 0, 1]], dtype = \"double\")\n",
    "\n",
    "\n",
    "    ## Distortion Coefficients - Zero for us since assuming no distortion\n",
    "    \n",
    "    dist_coeffs = np.zeros((4, 1))    \n",
    "\n",
    "\n",
    "    ## Getting face landmarks \n",
    "    landmarks = get_landmarks(image)\n",
    "\n",
    "    if landmarks is None:\n",
    "        return [(0, 0, 0)], image\n",
    "\n",
    "\n",
    "    ## Pose estimator\n",
    "    pose_estimator = HeadPoseEstimator(image_size=(height, width), mode=mode)\n",
    "    image_points = get_points_from_landmarks(landmarks, mode)\n",
    "    rotation_vector, translation_vector = pose_estimator.solve_pose(image_points)\n",
    "        \n",
    "    axis = np.float32([[350, 0, 0], \n",
    "\t                  [0, 350, 0], \n",
    "\t                  [0, 0, 350]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    imgpts, jac = cv2.projectPoints(axis, \n",
    "\t                                rotation_vector, \n",
    "\t                                translation_vector, \n",
    "\t                                camera_matrix, \n",
    "\t                                dist_coeffs)\n",
    "\n",
    "    modelpts, jac2 = cv2.projectPoints(model_points, \n",
    "\t                                   rotation_vector, \n",
    "\t                                   translation_vector, \n",
    "\t                                   camera_matrix, \n",
    "\t                                   dist_coeffs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t## Convert rotation vector to rotation matrix\n",
    "    rvec_matrix = cv2.Rodrigues(rotation_vector)[0]   \n",
    "\n",
    "\n",
    "\t## Projection Matrix\n",
    "    proj_matrix = np.hstack((rvec_matrix, translation_vector))\n",
    "\n",
    "\t## Getting Euler's angles (Roll, Pitch & Yaw)\n",
    "    eulerAngles = cv2.decomposeProjectionMatrix(proj_matrix)[6]\n",
    "\n",
    "\n",
    "\n",
    "\t## Pitch, Yaw & Roll\n",
    "    pitch, yaw, roll = [math.radians(_) for _ in eulerAngles]\n",
    "\n",
    "\t## To degrees\n",
    "    pitch = math.degrees(math.asin(math.sin(pitch)))\n",
    "    roll = -math.degrees(math.asin(math.sin(roll)))\n",
    "    yaw = math.degrees(math.asin(math.sin(yaw)))\n",
    "\n",
    "    face_direction = orientation_threshold(pitch, yaw)\n",
    "\n",
    "\n",
    "    rotate_degree = (str(int(roll)), str(int(pitch)), str(int(yaw)))\n",
    "\n",
    "    nose_x = int(image_points[0][0])\n",
    "    nose_y = int(image_points[0][1])    \n",
    "\n",
    "\n",
    "\t## Drawing lines\n",
    "\t## Conversion\n",
    "\t## X=R, Y=G, Z=B\n",
    "    cv2.line(image, (nose_x, nose_y), tuple(imgpts[1].ravel()), (0, 255, 0), 3) #GREEN - Y-axis - PITCH\n",
    "    cv2.line(image, (nose_x, nose_y), tuple(imgpts[2].ravel()), (255, 0, 0), 3) #BLUE - Z-axis - YAW\n",
    "    cv2.line(image, (nose_x, nose_y), tuple(imgpts[0].ravel()), (0, 0, 255), 3) #RED X-axis - ROLL    \n",
    "\n",
    "\n",
    "\t## Drawing feature points\n",
    "    for i, pnt in enumerate(image_points.tolist()):\n",
    "        cv2.circle(image, (int(pnt[0]), int(pnt[1])), 1, colors[i % 6], 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\t# Roll, Pitch & Yaw on image\n",
    "    if draw_values:\n",
    "        for j in range(len(rotate_degree)):\n",
    "            cv2.putText(image, ('{:03.2f}').format(float(rotate_degree[j])), \n",
    "                        (10, 30 + (30 * j)), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        1, (0, 0, 0), thickness=1, lineType=1)\n",
    "            \n",
    "\t        \n",
    "\n",
    "\t## Drawing direction instead of Roll, pitch, yaw values\n",
    "    if draw_direction:\n",
    "        for j in range(len(face_direction)):\n",
    "            cv2.putText(image, '{}'.format(str(face_direction[j])), \n",
    "                          (10, 30 + (30 * j)), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                          1, (255, 255, 255), thickness=2, lineType=1)\n",
    "\n",
    "\n",
    "\n",
    "    return [rotate_degree], image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,img=get_orientation('./test_images/mark.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('test.jpg',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Read Image\n",
    "im = cv2.imread('./test_images/mark.jpg')\n",
    "size = im.shape\n",
    "     \n",
    "#2D image points. If you change the image, you need to change vector\n",
    "image_points = np.array([\n",
    "                            (359, 391),     # Nose tip\n",
    "                            (399, 561),     # Chin\n",
    "                            (337, 297),     # Left eye left corner\n",
    "                            (513, 301),     # Right eye right corne\n",
    "                            (345, 465),     # Left Mouth corner\n",
    "                            (453, 469)      # Right mouth corner\n",
    "                        ], dtype=\"double\")\n",
    " \n",
    "# 3D model points.\n",
    "model_points = np.array([\n",
    "                            (0.0, 0.0, 0.0),             # Nose tip\n",
    "                            (0.0, -330.0, -65.0),        # Chin\n",
    "                            (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "                            (225.0, 170.0, -135.0),      # Right eye right corne\n",
    "                            (-150.0, -150.0, -125.0),    # Left Mouth corner\n",
    "                            (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "                         \n",
    "                        ])\n",
    " \n",
    " \n",
    "# Camera internals\n",
    " \n",
    "focal_length = size[1]\n",
    "center = (size[1]/2, size[0]/2)\n",
    "camera_matrix = np.array(\n",
    "                         [[focal_length, 0, center[0]],\n",
    "                         [0, focal_length, center[1]],\n",
    "                         [0, 0, 1]], dtype = \"double\"\n",
    "                         )\n",
    " \n",
    "print(\"Camera Matrix :\\n {0}\".format(camera_matrix))\n",
    " \n",
    "dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "(success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.cv2.SOLVEPNP_ITERATIVE)\n",
    " \n",
    "print(\"Rotation Vector:\\n {0}\".format(rotation_vector))\n",
    "print(\"Translation Vector:\\n {0}\".format(translation_vector)) \n",
    " \n",
    " \n",
    "# Project a 3D point (0, 0, 1000.0) onto the image plane.\n",
    "# We use this to draw a line sticking out of the nose\n",
    " \n",
    "#\n",
    "(nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    " \n",
    "for p in image_points:\n",
    "    cv2.circle(im, (int(p[0]), int(p[1])), 3, (0,0,255), -1)\n",
    " \n",
    " \n",
    "p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    " \n",
    "cv2.line(im, p1, p2, (255,0,0), 2)\n",
    " \n",
    "# Display image\n",
    "#plt.imshow(im)\n",
    "cv2.imwrite('pose_estimation1.png',im)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_env",
   "language": "python",
   "name": "video_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
